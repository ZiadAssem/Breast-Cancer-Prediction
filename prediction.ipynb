{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "%pip install seaborn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('breast-cancer.csv')\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Removing the ID column\n",
    "dataset.drop('id', axis=1, inplace=True)\n",
    "\n",
    "# 2 checking for missing values\n",
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Encoding the categorical data\n",
    "label_encoder = LabelEncoder()\n",
    "dataset['diagnosis'] = label_encoder.fit_transform(dataset['diagnosis'])\n",
    "print(dataset['diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Feature scaling\n",
    "features = dataset.columns.drop('diagnosis')\n",
    "scaler = StandardScaler()\n",
    "dataset[features] = scaler.fit_transform(dataset[features])\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Interquartile Range\n",
    "Q1 = dataset[features].quantile(0.25)\n",
    "Q2 = dataset[features].quantile(0.75)\n",
    "IQR = Q2 - Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the outliers\n",
    "outliers = dataset[(dataset[features] < (Q1 - 1.5 * IQR)) | (dataset[features] > (Q2 + 1.5 * IQR))].any(axis=1)\n",
    "cleaned_data = dataset[~outliers]\n",
    "print(f'Original data size: {len(dataset)}, Cleaned data size: {len(cleaned_data)}')\n",
    "print('no of outliers:', len(dataset) - len(cleaned_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the distribution of features (Histograms for each feature)\n",
    "for feature in features:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.histplot(cleaned_data[feature], kde=True, bins=30)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the correlation between features\n",
    "plt.figure(figsize=(20, 15))\n",
    "correlation_matrix = cleaned_data.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot to visualize relationships between two variables in the cleaned dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=cleaned_data, x='area_mean', y='texture_mean', hue='diagnosis')\n",
    "plt.title('Relationship Between Area Mean and Texture Mean (Cleaned)')\n",
    "plt.xlabel('Area Mean')\n",
    "plt.ylabel('Texture Mean')\n",
    "plt.legend(title='Diagnosis', labels=['Benign', 'Malignant'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, learning_curve\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cleaned_data.drop('diagnosis', axis=1)\n",
    "y = cleaned_data['diagnosis']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_decision_tree(X_train, X_test, y_train, y_test):\n",
    "    # Define parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'max_depth': [None, 5, 10, 15],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    # Initialize Decision Tree classifier\n",
    "    clf = DecisionTreeClassifier(random_state=42, criterion='entropy')\n",
    "\n",
    "    # Initialize GridSearchCV for hyperparameter tuning\n",
    "    grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best parameters and best estimator\n",
    "    best_params = grid_search.best_params_\n",
    "    best_clf = grid_search.best_estimator_\n",
    "\n",
    "    # Print best parameters\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = best_clf.predict(X_test)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Print performance metrics\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    y_pred_proba = best_clf.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot decision tree\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plot_tree(best_clf, filled=True, feature_names=X_train.columns, class_names=['Benign', 'Malignant'])\n",
    "    plt.show()\n",
    "\n",
    "    # Feature importance\n",
    "    feature_importance = best_clf.feature_importances_\n",
    "    sorted_indices = np.argsort(feature_importance)[::-1]\n",
    "    sorted_features = X_train.columns[sorted_indices]\n",
    "    sorted_importance = feature_importance[sorted_indices]\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x=sorted_importance, y=sorted_features, palette='viridis')\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title('Feature Importance Plot')\n",
    "    plt.show()\n",
    "\n",
    "    # Return best classifier\n",
    "    return best_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf = train_decision_tree(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(X_train, X_test, y_train, y_test):\n",
    "    # Define parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'p': [1, 2],\n",
    "        'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski']\n",
    "    }\n",
    "\n",
    "    # Initialize KNN classifier\n",
    "    knn = KNeighborsClassifier()\n",
    "\n",
    "    # Initialize GridSearchCV for hyperparameter tuning\n",
    "    grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best parameters and best estimator\n",
    "    best_params = grid_search.best_params_\n",
    "    best_knn = grid_search.best_estimator_\n",
    "\n",
    "    # Print best parameters\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = best_knn.predict(X_test)\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    tick_marks = np.arange(len(np.unique(y_test)))\n",
    "    plt.xticks(tick_marks, ['B', 'M'])\n",
    "    plt.yticks(tick_marks, ['B', 'M'])\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 ha=\"center\", va=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate ROC/AUC\n",
    "    y_scores = best_knn.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot learning curve\n",
    "    train_sizes, train_scores, test_scores = learning_curve(best_knn, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Print performance metrics\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "\n",
    "    \n",
    "\n",
    "    # Return best classifier\n",
    "    return best_knn, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classifier = knn(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_naive_bayes(X_train, X_test, y_train, y_test):\n",
    "   \n",
    "\n",
    "    # Initialize Gaussian Naive Bayes classifier\n",
    "    nb_classifier = GaussianNB()\n",
    "\n",
    "    # Train the classifier\n",
    "    nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "     # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    tick_marks = np.arange(len(np.unique(y_test)))\n",
    "    plt.xticks(tick_marks, ['Benign', 'Malignant'])\n",
    "    plt.yticks(tick_marks, ['Benign', 'Malignant'])\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 ha=\"center\", va=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    labels = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    values = [accuracy, precision, recall, f1]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(labels, values, color=['blue', 'green', 'red', 'purple'])\n",
    "    plt.title('Performance Metrics')\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0, 1)  # Set y-axis limit from 0 to 1\n",
    "    plt.show()\n",
    "    # Print performance metrics\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    \n",
    "\n",
    "    # Return trained classifier\n",
    "    return nb_classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = train_naive_bayes(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def svm(X_train, X_test, y_train, y_test):\n",
    "    # Define parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': [1, 0.1, 0.01, 0.001],\n",
    "        'kernel': ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "    }\n",
    "\n",
    "    # Initialize SVM classifier\n",
    "    svm_classifier = SVC()\n",
    "\n",
    "    # Initialize GridSearchCV for hyperparameter tuning\n",
    "    grid_search = GridSearchCV(svm_classifier, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best parameters and best estimator\n",
    "    best_params = grid_search.best_params_\n",
    "    best_svm = grid_search.best_estimator_\n",
    "\n",
    "    # Print best parameters\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = best_svm.predict(X_test)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Print performance metrics\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    \n",
    "       # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    tick_marks = np.arange(len(np.unique(y_test)))\n",
    "    plt.xticks(tick_marks, ['B', 'M'])\n",
    "    plt.yticks(tick_marks, ['B', 'M'])\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # Return best classifier\n",
    "    return best_svm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = svm(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def logistic_regression(X_train, X_test, y_train, y_test):\n",
    "    model = LogisticRegression(random_state=0,max_iter=10000)\n",
    "\n",
    "    # Train the classifier on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    y_true_str = [\"Benign\" if label == 0 else \"Malignant\" for label in y_test]\n",
    "    y_pred_str = [\"Benign\" if label == 0 else \"Malignant\" for label in y_pred]\n",
    "\n",
    "\n",
    "    # Evaluate the performance of the classifier\n",
    "    print(\"Accuracy:\", accuracy_score(y_true_str, y_pred_str))\n",
    "    print(\"Precision:\", precision_score(y_true_str, y_pred_str, pos_label=\"Malignant\"))\n",
    "    print(\"Recall:\", recall_score(y_true_str, y_pred_str, pos_label=\"Malignant\"))\n",
    "    print(\"F1 Score:\", f1_score(y_true_str, y_pred_str, pos_label=\"Malignant\"))\n",
    "\n",
    "\n",
    "    # Generate a confusion matrix plot\n",
    "    cm = confusion_matrix(y_true_str, y_pred_str, labels=[\"Benign\", \"Malignant\"])\n",
    "    cm_labels = {\"Benign\": \"Benign\", \"Malignant\": \"Malignant\"}\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=cm_labels.values(), yticklabels=cm_labels.values(),\n",
    "                cbar=False, annot_kws={\"fontsize\": 12}, linewidths=.5, linecolor='lightgray')\n",
    "    cbar = ax.figure.colorbar(ax.collections[0])\n",
    "    cbar.ax.tick_params(labelsize=12)\n",
    "    plt.title('Confusion Matrix', fontsize=14)\n",
    "    plt.xlabel('Predicted label', fontsize=12)\n",
    "    plt.ylabel('True label', fontsize=12)\n",
    "\n",
    "    # Generate an ROC curve plot\n",
    "    y_scores = model.decision_function(X_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fig2, ax2 = plt.subplots(figsize=(6, 6))\n",
    "    ax2.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    ax2.set_xlim([0.0, 1.0])\n",
    "    ax2.set_ylim([0.0, 1.05])\n",
    "    ax2.set_xlabel('False Positive Rate')\n",
    "    ax2.set_ylabel('True Positive Rate')\n",
    "    ax2.set_title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    ax2.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "    # Generate a feature importance plot (for models with coefficients available)\n",
    "    if hasattr(model, 'coef_'):\n",
    "        coefs = model.coef_.ravel()\n",
    "        if len(coefs) == 5:\n",
    "            names = ['symmetry_se', 'smoothness_mean', 'texture_se', 'symmetry_worst', 'compactness_se']\n",
    "        else:\n",
    "            names = X_train.columns\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(names, coefs)\n",
    "        plt.title('Feature Importance')\n",
    "        plt.ylabel('Coefficient Value')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "\n",
    "    # Show all plots\n",
    "    fig2, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=cm_labels.values(), yticklabels=cm_labels.values(),\n",
    "                cbar=False, annot_kws={\"fontsize\": 12}, linewidths=.5, linecolor='lightgray', ax=ax1)\n",
    "    ax1.set_title('Confusion Matrix', fontsize=14)\n",
    "    ax1.set_xlabel('Predicted label', fontsize=12)\n",
    "    ax1.set_ylabel('True label', fontsize=12)\n",
    "    ax2.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    ax2.set_xlim([0.0, 1.0])\n",
    "    ax2.set_ylim([0.0, 1.05])\n",
    "    ax2.set_xlabel('False Positive Rate')\n",
    "    ax2.set_ylabel('True Positive Rate')\n",
    "    ax2.set_title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    ax2.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_forest(X_train, X_test, y_train, y_test):\n",
    "    # Define parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [5, 10, 15, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    # Initialize Random Forest classifier\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # Initialize GridSearchCV for hyperparameter tuning\n",
    "    grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best parameters and best estimator\n",
    "    best_params = grid_search.best_params_\n",
    "    best_rf = grid_search.best_estimator_\n",
    "\n",
    "    # Print best parameters\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = best_rf.predict(X_test)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Print performance metrics\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    tick_marks = np.arange(len(np.unique(y_test)))\n",
    "    plt.xticks(tick_marks, ['B', 'M'])\n",
    "    plt.yticks(tick_marks, ['B', 'M'])\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    #plot feature importance\n",
    "    importances = best_rf.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(X_train.shape[1]), importances[indices])\n",
    "    plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)\n",
    "    plt.title(\"Feature Importance\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Return best classifier\n",
    "    return best_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest(X_train, X_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
